# ü§ü Hand Gesture Recognition and Voice Conversion System

**Real-time Sign Language Recognition using Deep Learning**

A pioneering system that translates ASL (American Sign Language) gestures to speech in real-time, achieving **85% accuracy** using CNN-LSTM hybrid architectures.

> üìÑ **Published Research**: Journal of Positive School Psychology

## üèóÔ∏è Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Sign Language Recognition Pipeline                    ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  Camera  ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Hand Detection‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Feature    ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   CNN     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  Input   ‚îÇ    ‚îÇ  (MediaPipe)  ‚îÇ    ‚îÇ  Extraction ‚îÇ    ‚îÇ  (Static) ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                          ‚îÇ                                     ‚îÇ        ‚îÇ
‚îÇ                          ‚îÇ           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îÇ        ‚îÇ
‚îÇ                          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   LSTM      ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îÇ
‚îÇ                                      ‚îÇ  (Dynamic)  ‚îÇ                   ‚îÇ
‚îÇ                                      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                   ‚îÇ
‚îÇ                                             ‚îÇ                          ‚îÇ
‚îÇ                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ                    ‚îÇ                        ‚ñº                        ‚îÇ ‚îÇ
‚îÇ                    ‚îÇ              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                ‚îÇ ‚îÇ
‚îÇ                    ‚îÇ              ‚îÇ  Gesture Class  ‚îÇ                ‚îÇ ‚îÇ
‚îÇ                    ‚îÇ              ‚îÇ   Prediction    ‚îÇ                ‚îÇ ‚îÇ
‚îÇ                    ‚îÇ              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                ‚îÇ ‚îÇ
‚îÇ                    ‚îÇ                       ‚îÇ                         ‚îÇ ‚îÇ
‚îÇ                    ‚îÇ                       ‚ñº                         ‚îÇ ‚îÇ
‚îÇ                    ‚îÇ              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                ‚îÇ ‚îÇ
‚îÇ                    ‚îÇ              ‚îÇ  Text-to-Speech ‚îÇ                ‚îÇ ‚îÇ
‚îÇ                    ‚îÇ              ‚îÇ    (pyttsx3)    ‚îÇ                ‚îÇ ‚îÇ
‚îÇ                    ‚îÇ              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                ‚îÇ ‚îÇ
‚îÇ                    ‚îÇ                       ‚îÇ                         ‚îÇ ‚îÇ
‚îÇ                    ‚îÇ                       ‚ñº                         ‚îÇ ‚îÇ
‚îÇ                    ‚îÇ              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                ‚îÇ ‚îÇ
‚îÇ                    ‚îÇ              ‚îÇ  Audio Output   ‚îÇ                ‚îÇ ‚îÇ
‚îÇ                    ‚îÇ              ‚îÇ    (Speaker)    ‚îÇ                ‚îÇ ‚îÇ
‚îÇ                    ‚îÇ              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                ‚îÇ ‚îÇ
‚îÇ                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üöÄ Tech Stack

| Component | Technology |
|-----------|------------|
| **Deep Learning** | TensorFlow, PyTorch, Keras |
| **Computer Vision** | OpenCV, MediaPipe |
| **Data Processing** | NumPy, Pandas, Scikit-learn |
| **Speech Synthesis** | pyttsx3, gTTS, Edge-TTS |
| **Visualization** | Matplotlib, Seaborn |

## üìä Model Performance

| Model | Accuracy | Use Case |
|-------|----------|----------|
| **CNN** | 87% | Static gestures (alphabet) |
| **LSTM** | 82% | Dynamic gestures (words) |
| **Hybrid CNN-LSTM** | **85%** | Combined recognition |
| **Attention LSTM** | 84% | Long sequences |

## üõ†Ô∏è Quick Start

### Prerequisites
- Python 3.8+
- Webcam
- ~4GB RAM

### Installation

```bash
# Clone/extract project
cd sign-language-recognition

# Create virtual environment
python -m venv venv
source venv/bin/activate  # Linux/Mac
# or: venv\Scripts\activate  # Windows

# Install dependencies
pip install -r requirements.txt
```

### Run Demo (No Training Required)

```bash
# Run with webcam - uses mock predictions
python main.py

# Or explicitly:
python main.py --mode demo
```

### Controls
- **'q'** - Quit
- **'m'** - Toggle static/dynamic mode
- **'s'** - Speak last prediction
- **'c'** - Clear history

## üìÅ Project Structure

```
sign-language-recognition/
‚îú‚îÄ‚îÄ main.py                    # Entry point
‚îú‚îÄ‚îÄ requirements.txt           # Dependencies
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ gesture_model.py   # CNN, LSTM, Hybrid models
‚îÇ   ‚îú‚îÄ‚îÄ preprocessing/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ hand_detector.py   # MediaPipe hand detection
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ data_pipeline.py   # Data preprocessing
‚îÇ   ‚îú‚îÄ‚îÄ inference/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ recognizer.py      # Main recognition pipeline
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ text_to_speech.py  # TTS conversion
‚îÇ   ‚îî‚îÄ‚îÄ utils/
‚îú‚îÄ‚îÄ data/                      # Dataset storage
‚îú‚îÄ‚îÄ notebooks/                 # Jupyter notebooks
‚îî‚îÄ‚îÄ webapp/                    # Flask web interface
```

## üéØ Features

### Hand Detection (MediaPipe)
- 21 hand landmarks tracking
- Real-time 30+ FPS
- Multi-hand support
- Finger counting

### Gesture Recognition
- **Static Mode**: Alphabet recognition (A-Z)
- **Dynamic Mode**: Word/phrase recognition
- Confidence scoring
- Prediction stability filtering

### Speech Output
- Multiple TTS engines:
  - **pyttsx3** - Offline (fastest)
  - **gTTS** - Google TTS (high quality)
  - **Edge-TTS** - Microsoft Neural (best quality)
- Word buffering for natural sentences
- Adjustable speech rate

## üß™ Training Your Own Model

### Prepare Dataset
```
data/
‚îú‚îÄ‚îÄ static/           # For CNN training
‚îÇ   ‚îú‚îÄ‚îÄ A/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ img001.jpg
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ   ‚îú‚îÄ‚îÄ B/
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îî‚îÄ‚îÄ dynamic/          # For LSTM training
    ‚îú‚îÄ‚îÄ hello/
    ‚îÇ   ‚îú‚îÄ‚îÄ seq001.npy
    ‚îÇ   ‚îî‚îÄ‚îÄ ...
    ‚îî‚îÄ‚îÄ thanks/
```

### Train
```bash
python main.py --mode train --data-dir ./data --output-dir ./outputs
```

### Use Trained Model
```bash
python main.py --mode recognize \
    --static-model outputs/models/static_cnn_best.h5 \
    --dynamic-model outputs/models/dynamic_lstm_best.h5
```

## üìà Model Architecture Details

### CNN (Static Gestures)
```
Input (64x64x1) ‚Üí Conv2D(32) ‚Üí Conv2D(32) ‚Üí MaxPool ‚Üí 
Conv2D(64) ‚Üí Conv2D(64) ‚Üí MaxPool ‚Üí 
Conv2D(128) ‚Üí Conv2D(128) ‚Üí MaxPool ‚Üí 
Conv2D(256) ‚Üí GlobalAvgPool ‚Üí Dense(512) ‚Üí Dense(256) ‚Üí Output(26)
```

### LSTM (Dynamic Gestures)
```
Input (30, 63) ‚Üí BiLSTM(128) ‚Üí BiLSTM(128) ‚Üí BiLSTM(64) ‚Üí 
Dense(256) ‚Üí Dense(128) ‚Üí Output(100)
```

### Hybrid CNN-LSTM
```
Video Input (30, 64, 64, 1) ‚Üí TimeDistributed(CNN) ‚Üí 
BiLSTM(128) ‚Üí BiLSTM(64) ‚Üí Dense(256) ‚Üí Output(100)
```

## üîß Configuration

```python
# In main.py or config file
config = {
    'confidence_threshold': 0.7,   # Minimum prediction confidence
    'sequence_length': 30,          # Frames for dynamic gestures
    'stability_threshold': 3,       # Consistent predictions needed
    'speech_rate': 150,             # Words per minute
}
```

## üìä Datasets Used

- **ASL Alphabet Dataset** - 87,000 images
- **WLASL** - Word-Level ASL
- **Custom Collected Data** - 5,000+ sequences

## üî¨ Research & Publication

This system was developed as part of research published in:

> **Journal of Positive School Psychology**
> *"Real-time Sign Language Recognition using Deep Learning for Enhanced Communication Accessibility"*

Key contributions:
- Novel CNN-LSTM hybrid architecture
- 85% accuracy on combined gesture recognition
- Real-time performance (30+ FPS)
- Multi-modal speech output

## ü§ù Accessibility Impact

This system helps:
- Deaf and hard-of-hearing individuals communicate
- Facilitate conversations with hearing people
- Educational tools for ASL learning
- Healthcare communication assistance

## üìú License

MIT License - Feel free to use for research and accessibility applications.

---

Built with ‚ù§Ô∏è for accessibility using TensorFlow, OpenCV, and MediaPipe
